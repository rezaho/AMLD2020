{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab Setup\n",
    "---\n",
    "\n",
    "Make sure to select GPU in Runtime > Change runtime type > Hardware accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title << Setup Google Colab by running this cell {display-mode: \"form\"}\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/pacm/rl-workshop.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"rl-workshop/agents\" \"rl-workshop/env\" \"rl-workshop/helpers\" \"rl-workshop/videos\" .\n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"rl-workshop/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File structure\n",
    "---\n",
    "\n",
    "Take a moment to look at the repo structure\n",
    "\n",
    "```\n",
    "├── Notebooks, Readme, packages ..\n",
    "├── agents: RL agents implementation\n",
    "│   ├── curiosity.py\n",
    "│   ├── dqn.py\n",
    "│   ├── qlearning.py\n",
    "│   └── random.py\n",
    "├── env: Workshop RL environment\n",
    "│   ├── 16ShipCollection.png\n",
    "│   ├── Inconsolata-Bold.ttf, ..\n",
    "│   └── env.py\n",
    "├── helpers: Helpers to train, test, inspect agents\n",
    "│   └── rl_helpers.py\n",
    "└── videos: Save videos of your best agents here!\n",
    "    └── video.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env/env.py\n",
    "%run helpers/rl_helpers.py\n",
    "%run agents/dqn.py\n",
    "%run agents/qlearning.py\n",
    "%run agents/random.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might want to import other libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presentation of the RL environment\n",
    "---\n",
    "\n",
    "After creating the environment, you will need to call `reset()` to initalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = DeliveryDrones()\n",
    "env.env_params.update({\n",
    "    'n_drones': 3, 'rgb_render_rescale': 1.0,\n",
    "    'packets_factor': 3, 'dropzones_factor': 2, 'stations_factor': 2, 'skyscrapers_factor': 3\n",
    "})\n",
    "states = env.reset()\n",
    "\n",
    "# Render in text\n",
    "print(env.render(mode='ainsi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render as an RGB image\n",
    "Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presentation of the observations spaces\n",
    "---\n",
    "\n",
    "By default, the environment returns `ground` and `air` grids as observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations are returned after env.reset() or env.step() calls\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can inspect what's on the ground\n",
    "states['ground'].grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **observation wrappers** to produce states that can be used with RL agents. See OpenAI Gym code [here](https://github.com/openai/gym/blob/c6a97e17ee392b5bbfd297fb3b49ab86b6d94836/gym/core.py#L252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation wrappers for Q-table RL methods:\n",
    "# - CompassQTable, CompassChargeQTable, LidarCompassQTable, LidarCompassChargeQTable\n",
    "env = LidarCompassChargeQTable(DeliveryDrones())\n",
    "env.env_params.update({\n",
    "    'n_drones': 3, 'rgb_render_rescale': 1.0,\n",
    "    'packets_factor': 3, 'dropzones_factor': 2, 'stations_factor': 2, 'skyscrapers_factor': 3\n",
    "})\n",
    "states = env.reset()\n",
    "print('states:', env.observation(states))\n",
    "Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{drone: env.format_state(state) for drone, state in states.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presentation of WindowedGridView\n",
    "---\n",
    "\n",
    "This is the \"official\" wrapper for the competition\n",
    "\n",
    "```\n",
    "Observation wrapper: (N, N, 6) numerical arrays with location of\n",
    "(0) drones         marked with                   1 / 0 otherwise\n",
    "(1) packets        marked with                   1 / 0 otherwise\n",
    "(2) dropzones      marked with                   1 / 0 otherwise\n",
    "(3) stations       marked with                   1 / 0 otherwise\n",
    "(4) drones charge  marked with   charge level 0..1 / 0 otherwise\n",
    "(5) obstacles      marked with                   1 / 0 otherwise\n",
    "Where N is the size of the window, i the number of drones\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WindowedGridView(DeliveryDrones(), radius=2)\n",
    "states = env.reset()\n",
    "Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{drone: env.format_state(state) for drone, state in states.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[0][:, :, 5] # Obstacles from the perspective of drone 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and run agents\n",
    "---\n",
    "\n",
    "After creating your agents, you can run them with the `test_agents()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and setup the environment\n",
    "env = CompassQTable(DeliveryDrones())\n",
    "env.env_params.update({\n",
    "    'n_drones': 3, 'rgb_render_rescale': 1.0,\n",
    "    'pickup_reward': 0, 'delivery_reward': 1, 'crash_reward': -1, 'charge_reward': -0.1\n",
    "})\n",
    "states = env.reset()\n",
    "\n",
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agents\n",
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "\n",
    "# Print rewards\n",
    "for drone_index, rewards in rewards_log.items():\n",
    "    print('Drone {} rewards: {} ..'.format(drone_index, rewards[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the rewards with the helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_rewards(\n",
    "    rewards_log,\n",
    "    events={'pickup': [1], 'crash': [-1]}, # Optional, default: pickup/crash ±1\n",
    "    drones_labels={0: 'My drone'}, # Optional, default: drone index \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train agents\n",
    "---\n",
    "\n",
    "To train your agents, you will need to use the `MultiAgentTrainer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and setup the environment\n",
    "env = CompassQTable(DeliveryDrones())\n",
    "env.env_params.update({'n_drones': 3, 'skyscrapers_factor': 0, 'charge_reward': 0, 'discharge': 0})\n",
    "states = env.reset()\n",
    "\n",
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = QLearningAgent(env, gamma=0.9, alpha=0.1, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01)\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
    "\n",
    "# Train with different grids\n",
    "trainer.train(5000)\n",
    "\n",
    "# Print rewards\n",
    "for drone_index, rewards in trainer.rewards_log.items():\n",
    "    print('Drone {} rewards: {} ..'.format(drone_index, rewards[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize training with helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_rewards(\n",
    "    trainer.rewards_log,\n",
    "    events={'pickup': [1], 'crash': [-1]}, # Optional, default: pickup/crash ±1\n",
    "    drones_labels={0: 'My drone'}, # Optional, default: drone index \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_rewards(trainer.rewards_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test agents\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(rewards_log, drones_labels={0: 'My drone'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a \"run\"\n",
    "---\n",
    "\n",
    "Share videos of your best agents! `#AMLD2020`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('videos', 'intro-run.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ColabVideo(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
